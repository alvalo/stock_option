{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import date, timedelta\n",
    "from progress.bar import Bar, ChargingBar\n",
    "import os, time, random\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "attempting to log in as alvar0291\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "\n",
    "LOGIN_URL = \"https://app.flowalgo.com/users/login\"\n",
    "\n",
    "\n",
    "def get_authenticity_token(html):\n",
    "    soup = BeautifulSoup(html, \"html.parser\")\n",
    "    token = soup.find('input', attrs={'name': 'login_attempt_id'})\n",
    "    if not token:\n",
    "        print('could not find `authenticity_token` on login form')\n",
    "    return token.get('value').strip()\n",
    "\n",
    "email = \"********\" \n",
    "password = \"******\" \n",
    "\n",
    "payload = {\n",
    "    'amember_login': email,\n",
    "    'amember_pass': password,\n",
    "    'utf8': '&#x2713;',\n",
    "}\n",
    "\n",
    "session = requests.Session()\n",
    "session.headers = {'User-Agent': ('Mozilla / 5.0 (Macintosh; Intel Mac OS X 10_10_1) AppleWebKit / 537.36 (KHTML, como Gecko) Chrome / 39.0.2171.95 Safari / 537.36')}\n",
    "response = session.get(LOGIN_URL)\n",
    "\n",
    "token = get_authenticity_token(response.text)\n",
    "payload.update({\n",
    "    'login_attempt_id': token,\n",
    "})\n",
    "\n",
    "print(f\"attempting to log in as {email}\")\n",
    "p = session.post(LOGIN_URL, data=payload) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sacar_soup(session,url_page):\n",
    "    with session as session:\n",
    "        post = p\n",
    "        r = session.get(url_page)\n",
    "        soup = BeautifulSoup(r.text, \"html.parser\")\n",
    "        return soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crear_tabla(soup):\n",
    "    #date\n",
    "    date = [item.get_text(strip=True) for item in soup.select(\"div.date\")]\n",
    "    date = date[1:-1]\n",
    "    #time\n",
    "    time = [item.get_text(strip=True) for item in soup.select(\"div.time\")]\n",
    "    time = time[1:-1]\n",
    "    #ticker\n",
    "    ticker = [item.get_text(strip=True) for item in soup.select(\"div.ticker\")]\n",
    "    ticker = ticker[1:-1]\n",
    "    #expiry\n",
    "    expiry = [item.get_text(strip=True) for item in soup.select(\"div.expiry\")]\n",
    "    expiry = expiry[1:]\n",
    "    for i in range(0,len(expiry)):\n",
    "        expiry[i] = expiry[i][6:]\n",
    "    #strike\n",
    "    strike = [item.get_text(strip=True) for item in soup.select(\"div.strike\")]\n",
    "    strike = strike[1:]\n",
    "    for i in range(0,len(strike)):\n",
    "        strike[i] = strike[i][6:]\n",
    "    #C/P\n",
    "    c_p = [item.get_text(strip=True) for item in soup.select(\"div.contract-type\")]\n",
    "    c_p = c_p[1:]\n",
    "    for i in range(0,len(c_p)):\n",
    "        c_p[i] = c_p[i][3:]\n",
    "    #spot\n",
    "    spot = [item.get_text(strip=True) for item in soup.select(\"div.ref\")]\n",
    "    spot = spot[1:-1]\n",
    "    for i in range(0,len(spot)):\n",
    "        spot[i] = spot[i][10:]\n",
    "    #details\n",
    "    details = [item.get_text(strip=True) for item in soup.select(\"div.details\")]\n",
    "    volume_operation = []\n",
    "    price_operation = []\n",
    "    details = details[1:]\n",
    "    for i in range(0,len(details)):\n",
    "        details[i] = details[i][21:]\n",
    "    for i in range(0,len(details)):\n",
    "        _ = details[i].split('@')\n",
    "        voloperation = _[0].strip()\n",
    "        priceoperation = _[1].strip()\n",
    "        volume_operation.append(voloperation)\n",
    "        price_operation.append(priceoperation)\n",
    "    for i in range(0,len(volume_operation)):\n",
    "        volume_operation[i] = volume_operation[i].replace(',', '')\n",
    "    #Type\n",
    "    tipo = [item.get_text(strip=True) for item in soup.select(\"div.type\")]\n",
    "    tipo = tipo[1:]\n",
    "    #vol\n",
    "    vol = [item.get_text(strip=True) for item in soup.select(\"div.volume\")]\n",
    "    vol = vol[1:]\n",
    "    for i in range(0,len(vol)):\n",
    "        vol[i] = vol[i].replace(',', '')\n",
    "    #OI\n",
    "    open_interest = [item.get_text(strip=True) for item in soup.select(\"div.open-interest\")]\n",
    "    open_interest = open_interest[1:]\n",
    "    for i in range(0,len(open_interest)):\n",
    "        open_interest[i] = open_interest[i].replace(',', '')\n",
    "\n",
    "    \n",
    "    data_tuples = list(zip(date,time,ticker,expiry,strike,c_p,spot,volume_operation,price_operation,tipo,vol,open_interest))\n",
    "    BBDD = pd.DataFrame(data_tuples,\n",
    "                   columns = ['date', \n",
    "                              'time', \n",
    "                              'ticker', \n",
    "                              'expiry', \n",
    "                              'strike', \n",
    "                              'c_p', 'spot', \n",
    "                              'volume_operation', \n",
    "                              'price_operation', \n",
    "                              'tipo', \n",
    "                              'vol', \n",
    "                              'open_interest'])\n",
    "    BBDD[['strike', \n",
    "          'spot', \n",
    "          'volume_operation', \n",
    "          'price_operation', \n",
    "          'vol', \n",
    "          'open_interest']] = BBDD[['strike', \n",
    "                                    'spot', \n",
    "                                    'volume_operation', \n",
    "                                    'price_operation', \n",
    "                                    'vol', \n",
    "                                    'open_interest']].apply(pd.to_numeric)\n",
    "    BBDD['money'] = BBDD['volume_operation'] * BBDD['price_operation'] \n",
    "    BBDD['date'] = BBDD['date'] + ' ' + BBDD['time']\n",
    "    BBDD = BBDD.drop(['time'], axis = 1)\n",
    "    BBDD['date'] = pd.to_datetime(BBDD['date'], format = '%m/%d/%y %I:%M %p')\n",
    "    BBDD['date'] = pd.to_datetime(BBDD['date'], format = '%d/%m/%y %H:%M')\n",
    "    BBDD['expiry'] = pd.to_datetime(BBDD['expiry'], format = '%m/%d/%y')\n",
    "    BBDD['expiry'] = pd.to_datetime(BBDD['expiry'], format = '%d/%m/%y')\n",
    "    BBDD.set_index('date', inplace = True)\n",
    "    \n",
    "    \n",
    "    return BBDD\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_page = 'https://app.flowalgo.com/historical-flow/?tickers=&start=&end=&minsize=&show=500'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = sacar_soup(session,url_page)\n",
    "BBDD = crear_tabla(soup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tickers = list(BBDD['ticker'])\n",
    "tickers = np.unique(tickers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['AAPL', 'ABBV', 'ABNB', 'AFRM', 'AGNC', 'AHT', 'ALGN', 'AMC',\n",
       "       'AMD', 'AMZN', 'APA', 'API', 'AQUA', 'ARKK', 'ATOS', 'ATVI',\n",
       "       'AVGO', 'AYX', 'BABA', 'BAC', 'BB', 'BBIG', 'BGFV', 'BHF', 'BMY',\n",
       "       'BP', 'BTU', 'BX', 'C', 'CAR', 'CCJ', 'CCL', 'CDE', 'CHWY', 'CKPT',\n",
       "       'CLF', 'CLOV', 'CMCSA', 'CNK', 'CNST', 'COG', 'CRBP', 'CRIS',\n",
       "       'CRSR', 'CSCO', 'CVM', 'CWH', 'DAL', 'DBX', 'DDD', 'DIS', 'DOCU',\n",
       "       'DPW', 'DRI', 'EBAY', 'EDU', 'EEM', 'ELY', 'ENB', 'ET', 'EVFM',\n",
       "       'EWP', 'EWZ', 'F', 'FAII', 'FB', 'FCX', 'FEYE', 'FOA', 'FSLR',\n",
       "       'FSR', 'FUBO', 'GE', 'GEO', 'GGAL', 'GLD', 'GME', 'GOGO', 'GTT',\n",
       "       'HL', 'IAA', 'IDEX', 'INTC', 'IRM', 'IVR', 'IWN', 'JNPR', 'KBH',\n",
       "       'KHC', 'KLR', 'KO', 'KODK', 'KSS', 'LEV', 'LH', 'LI', 'LOTZ',\n",
       "       'LQD', 'MA', 'MAC', 'MCD', 'MGI', 'MGNI', 'MNMD', 'MOMO', 'MRK',\n",
       "       'MSFT', 'MT', 'MU', 'MUDS', 'MUX', 'MX', 'NCLH', 'NEE', 'NFLX',\n",
       "       'NGL', 'NIO', 'NOK', 'NVDA', 'OCGN', 'ORCL', 'OTLY', 'OXY', 'PAA',\n",
       "       'PBR', 'PCG', 'PFE', 'PLTR', 'PLUG', 'PSFE', 'PSTH', 'QCOM', 'QQQ',\n",
       "       'QS', 'RAMP', 'RBLX', 'RIG', 'RLX', 'RMO', 'ROOT', 'RRD', 'RY',\n",
       "       'SAGE', 'SCHW', 'SDC', 'SENS', 'SIRI', 'SLV', 'SNAP', 'SOLO',\n",
       "       'SOS', 'SPX', 'SPY', 'SQ', 'SRAX', 'STAY', 'STEM', 'SVXY', 'TBT',\n",
       "       'TEN', 'TEVA', 'TLRY', 'TME', 'TPX', 'TS', 'TSLA', 'TUR', 'TWTR',\n",
       "       'UBER', 'UWMC', 'UXIN', 'VALE', 'VRM', 'VTNR', 'VXRT', 'WISH', 'X',\n",
       "       'XEC', 'XLC', 'XLY', 'XOM', 'XPEV', 'XSPA', 'ZM', 'ZNGA'],\n",
       "      dtype='<U5')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tickers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdate = date(2017,6,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "edate = date(2018,12,31)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "dias = pd.date_range(sdate,edate-timedelta(days=1),freq='d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "url1 = 'https://app.flowalgo.com/historical-flow/?tickers='\n",
    "url2 = '&start='\n",
    "url3 = '%2F'\n",
    "url4 = '&end='\n",
    "url5 = '&minsize=&show=500'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "dia = list(dias.strftime('%d'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "mes = list(dias.strftime('%m'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "año = list(dias.strftime('%Y'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "lista_url = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "185"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tickers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(dia)+1):\n",
    "    if i<len(dia)-1:\n",
    "        diario = dia[i]\n",
    "        mensual = mes[i]\n",
    "        anual = año[i]\n",
    "        dia_despues = dia[i+1]\n",
    "        mes_despues = mes[i+1]\n",
    "        año_despues = año[i+1]\n",
    "        for t in range(len(tickers)):\n",
    "            ticker = tickers[t]\n",
    "            url = (url1+str(ticker)+url2+str(mensual)+url3+str(diario)+url3+str(anual)+url4+str(mes_despues)+url3+str(dia_despues)+url3+str(año_despues)+url5)\n",
    "            lista_url.append(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(dia)+1):\n",
    "    if i<len(dia)-1:\n",
    "        diario = dia[i]\n",
    "        mensual = mes[i]\n",
    "        anual = año[i]\n",
    "        dia_despues = dia[i+1]\n",
    "        mes_despues = mes[i+1]\n",
    "        año_despues = año[i+1]\n",
    "        ticker = 'BAC'\n",
    "        url = (url1+str(ticker)+url2+str(mensual)+url3+str(diario)+url3+str(anual)+url4+str(mes_despues)+url3+str(dia_despues)+url3+str(año_despues)+url5)\n",
    "        lista_url.append(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "lo_consegui = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 576/576 [11:31<00:00,  1.20s/it]\n"
     ]
    }
   ],
   "source": [
    "for i in tqdm(range(len(lista_url))):\n",
    "    url_page = lista_url[i]\n",
    "    soup = sacar_soup(session,url_page)\n",
    "    BBDD = crear_tabla(soup)\n",
    "    unir = [BBDD, lo_consegui]\n",
    "    lo_consegui = pd.concat(unir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "lo_consegui.to_csv('datosBAC.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
